\chapter{Introduction}

Rough Outline
\begin{enumerate}
	\item background
	\item state of the art
	\item problem statement and scope
	\item Analysis and methodology. \\
		  Start from S2S RNN and discuss how this applies in general to NMT.
		  Introduce Transformer architecture as SOTA in NMT.
		  
\end{enumerate}


\section{Background}
\par
The modern distribution network has changed more over the last ten years than it has in the previous hundred.
In the past, generation and load were largely separate; power was generated at large stations, and power was consumed by customers after traversing the transmission and distribution networks. 
These days, power is still consumed in the distribution network, but is also generated and manipulated by distributed energy resources (DER). 
\par
DERs are controllable devices in the power network that generate, store, and consume load. 
This includes solar generation (PV), battery storage, and electric vehicles (EV). 
\par
The Tasmanian distribution network is forecast to experience significant increases in these technologies by 2025: \\
\begin{itemize}
	\item 600\% increase in battery storage capacity (from 100MWh to 600MWh) \cite{Jacobs2017}
	\item 170\% increase in PV installation capacity (from 130MW to 220MW) \cite{Jacobs2017}
	\item 39\% of new car sales with be EVs - the highest in the country \cite{AEMO2016}
\end{itemize}

This changing network presents an opportunity to maximize the use of existing assets by delaying the need for network augmentations, while also providing customers with a more reliable supply of power.
However, to achieve this requires sophisticated methods to optimize the power flow to and from the distributed resources \cite{Scott2014}.
\par
One requirement for optimization is an accurate forecast of day-ahead load at the feeder level.
Traditional load forecasting methods do not provide an adequate level of accuracy over a time period this short.
As a result, the optimization of the distributed resources is not as effective as it could be.
\par
To solve this, a neural network-based load forecasting system is proposed.
This system will be applied to Bruny Island, in southern Tasmania, as a case study.
Bruny Island currently has a high penetration of PV and battery technology which is optimized by the Network-aware Coordination (NAC) algorithm \cite{Evan2016}.
Additionally, it is constrained by its feeder during peak holiday periods, necessitating an on-island diesel generator.
This makes it a perfect case study to highlight the potential benefits that distributed resources can have in the network.
This project and case study is supported by TasNetworks.

\section{State of the art in load forecasting}
\todo[inline]{I'm not using any of the methods mentioned here - issue?}
\todo[inline]{
	Should each of these subsections be independent? 
	Should they relate to one another?
	Should they simply flow together as natural progressions?
}
\todo[inline]{Do I need all of these sections, or should I talk about RNN and Transformer exclusively?}
Lit review.

\subsection{Autoregressive integrated moving average}
Autoregressive integrated moving average (ARIMA) models, introduced by \citet{Box1970}, are used widely for time series forecasting \citep{Weron2006}.
The ARIMA model is an extension of the autoregressive moving average (ARMA) model, which is comprised of an autoregressive (AR) term and a moving average (MA) term.
\\
Consider a time series $X$, being drawn from some underlying process, with elements $X_{t}, X_{t-1}, ..., X_{0}$. 
\\
An autoregressive model is given by
\begin{equation}
X_{t} = \alpha_{1}X_{t-1} + \alpha_{2}X_{t-2} + \ldots + \alpha_{p}X_{t-p} + \epsilon_{t} = \epsilon_{t} + \sum_{i=1}^{p}\alpha_{i}X_{t-i}
\end{equation}
where $p$ is the order of the AR model, $\alpha$ are the parameters of the model, and $\epsilon_{t}$ is a random normally distributed error with zero mean and variance $\sigma^2$. 
\\ 
A moving average model is given by 
\begin{equation}
X_{t} = \theta_{1}\epsilon_{t-1} + \theta_{2}\epsilon_{t-2} + \ldots + \theta_{q}\epsilon_{t-q} + \epsilon_{t} = \epsilon_{t} + \sum_{i=1}^{q}\theta_{i}\epsilon_{t-i}
\end{equation}
where $q$ is the order of the MA model, $\theta$ are the parameters of the model, and $\epsilon_{t}$ is, again, a random normally distributed error with zero mean and variance $\sigma^2$.
\par
The AR and MA models are combined to form an ARMA model, given by 
\begin{equation}
X_{t} = \epsilon_{t} + \sum_{i=1}^{q}\theta_{i}\epsilon_{t-i} + \sum_{i=1}^{p}\alpha_{i}X_{t-i}
\end{equation}
\par
An ARMA model requires the time series, $X$, being forecast to come from a weakly stationary process.
That is, the mean and autocovariance of the the process do not change with time.
\par
The ARIMA model is an extension to the ARMA model that can be applied when non-stationarity is exhibited by the underlying process.
ARIMA applies differencing to the time series prior to applying the ARMA model to the new time series $\hat{X}$.
This differencing is applied such that $\hat{X}_{t} = X_{t} - X_{t-1}$ and is repeated $d$ times.
Differencing can sometimes be sufficient to make a series stationary, such that it is appropriate for forecasting with an ARMA model.
\par
In order to concisely articulate the ARIMA model, the following notation will be used. The backward shift operator, defined by $\textbf{B}^{m}X_{t} = X_{t-m}$. The difference operator, defined by $\nabla_{s} X_{t} = X_{t} - X_{t-s} = (1 - \textbf{B}^{s})X_{t}$. The parameter functions $\alpha_{p}(\textbf{B}) = 1 - \alpha_{1}\textbf{B} - \ldots - \alpha_{p}\textbf{B}^p$ and $\theta_{q}(\textbf{B}) = 1 + \theta_{1}\textbf{B} + \ldots + \theta_{q}\textbf{B}^q$.
\par
Using this notation, the ARIMA model can be represented as the following
\begin{equation}
\alpha_{p}(\textbf{B})(\nabla_{1}^{d}X_{t}) = \theta_{q}(\textbf{B})\epsilon_{t}
\end{equation}
This is identical to an ARMA model, except that $X_{t}$ has been replaced with a  version of itself, $\nabla_{1}^{d}X_{t}$, that has been differenced $d$ times.
The above model is denoted ARIMA($p,d,q$).
\par
Differencing with a lag of one, as applied by the differencing operator, is not always sufficient to make the series stationary even when performed multiple ($d$) times.
A further extension of the ARIMA model is the seasonal ARIMA (SARIMA) model, which takes into account seasonal patterns which require differencing with a lag greater than one, such as daily, weekly, and yearly seasonalities commonly found in electrical load time series.
\\
Denoted ARIMA$(p,d,q)\times(P,D,Q)s$, a SARIMA model can be represented as the following
\begin{equation}
\alpha_{p}(\textbf{B})A_{P}(\textbf{B}^{s})(\nabla_{1}^{d}\nabla_{s}^{D}X_{t}) = \theta_{q}(\textbf{B})\Theta_{Q}(\textbf{B}^{s})\epsilon_{t}
\end{equation}
Where $(p,d,q)$ are the parameters of the non-seasonal ARIMA component, $(P,Q,D)s$ are the parameters of the seasonal component, $A_{P}(\textbf{B}^{s})$ and $\Theta_{Q}(\textbf{B}^{s})$ are defined similarly to $\alpha_{p}(\textbf{B})$ and $\theta_{q}(\textbf{B})$, and $s$ is the number of periods over which the seasonality occurs (e.g. $s=7$ for weekly seasonality if the data is daily).
\par
The SARIMA model can be further extended to a SARIMA with exogenous variables (SARIMAX) model. 

\subsection{Support vector regression}
SVM

\subsection{Gradient boosting}
Extreme (Red Bull EXTREME) Gradient Boosting

\subsection{Clustering and similar day}
k-means is inappropriate for varying sized clusters but when your data's got too many dimensions to visualize no one's gonna stop you.

\subsection{Multilayer perceptron}
Naive, ez

\subsection{Convolutional NN}
"Hey, it works for images why not time series", the short novel by various authors.
something something positional embedding.

\subsection{Recurrent NN}
The real deal. S2S, too. Notably, transformer architecture does not get a mention here.

\section{Problem statement}
predict load blah blah horizon and intervals.

\section{Project scope}
good question - mostly inside the honours propsal if I remember correctly.