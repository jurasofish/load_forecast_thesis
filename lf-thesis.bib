% Encoding: UTF-8

@Article{Zheng2017,
  author    = {Huiting Zheng and Jiabin Yuan and Long Chen},
  title     = {Short-Term Load Forecasting Using {EMD}-{LSTM} Neural Networks with a Xgboost Algorithm for Feature Importance Evaluation},
  journal   = {Energies},
  year      = {2017},
  volume    = {10},
  number    = {8},
  month     = {aug},
  pages     = {1168},
  doi       = {10.3390/en10081168},
  abstract  = {Accurate load forecasting is an important issue for the reliable and efficient operation of a power system. This study presents a hybrid algorithm that combines similar days (SD) selection, empirical mode decomposition (EMD), and long short-term memory (LSTM) neural networks to construct a prediction model (i.e., SD-EMD-LSTM) for short-term load forecasting. The extreme gradient boosting-based weighted k-means algorithm is used to evaluate the similarity between the forecasting and historical days. The EMD method is employed to decompose the SD load to several intrinsic mode functions (IMFs) and residual. Separated LSTM neural networks were also employed to forecast each IMF and residual. Lastly, the forecasting values from each LSTM model were reconstructed. Numerical testing demonstrates that the SD-EMD-LSTM method can accurately forecast the electric load.},
  file      = {:papers/Short-Term Load Forecasting Using EMD-LSTM.pdf:PDF},
  publisher = {{MDPI} {AG}},
  review    = {This method uses a S2S LSTM RNN for load forecasting.
K-means clustering with xgboost is used to select a similar day for input to the network.
Intrinsic mode decomposition is used to split the forecast into seperate nerual networks which are then recombined at the end.
Authors claim it works far better than plain LSTM ( such as implemented by Marino)},
}

@Article{Marino2016,
  author      = {Daniel L. Marino and Kasun Amarasinghe and Milos Manic},
  title       = {Building Energy Load Forecasting using Deep Neural Networks},
  journal     = {CoRR},
  year        = {2016},
  date        = {2016-10-29},
  eprint      = {1610.09460v1},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  abstract    = {Ensuring sustainability demands more efficient energy management with minimized energy wastage. Therefore, the power grid of the future should provide an unprecedented level of flexibility in energy management. To that end, intelligent decision making requires accurate predictions of future energy demand/load, both at aggregate and individual site level. Thus, energy load forecasting have received increased attention in the recent past, however has proven to be a difficult problem. This paper presents a novel energy load forecasting methodology based on Deep Neural Networks, specifically Long Short Term Memory (LSTM) algorithms. The presented work investigates two variants of the LSTM: 1) standard LSTM and 2) LSTM-based Sequence to Sequence (S2S) architecture. Both methods were implemented on a benchmark data set of electricity consumption data from one residential customer. Both architectures where trained and tested on one hour and one-minute time-step resolution datasets. Experimental results showed that the standard LSTM failed at one-minute resolution data while performing well in one-hour resolution data. It was shown that S2S architecture performed well on both datasets. Further, it was shown that the presented methods produced comparable results with the other deep learning methods for energy forecasting in literature.},
  file        = {:papers/Building Energy Load Forecasting using Deep Neural.pdf:PDF},
  keywords    = {cs.NE},
}

@Article{Bianchi2017,
  author        = {Filippo Maria Bianchi and Enrico Maiorino and Michael C. Kampffmeyer and Antonello Rizzi and Robert Jenssen},
  title         = {An overview and comparative analysis of Recurrent Neural Networks for Short Term Load Forecasting},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1705.04378},
  eprint        = {1705.04378},
  abstract    = {The key component in forecasting demand and consumption of resources in a supply network is an accurate prediction of real-valued time series. Indeed, both service interruptions and resource waste can be reduced with the implementation of an effective forecasting system. Significant research has thus been devoted to the design and development of methodologies for short term load forecasting over the past decades. A class of mathematical models, called Recurrent Neural Networks, are nowadays gaining renewed interest among researchers and they are replacing many practical implementation of the forecasting systems, previously based on static methods. Despite the undeniable expressive power of these architectures, their recurrent nature complicates their understanding and poses challenges in the training procedures. Recently, new important families of recurrent architectures have emerged and their applicability in the context of load forecasting has not been investigated completely yet. In this paper we perform a comparative study on the problem of Short-Term Load Forecast, by using different classes of state-of-the-art Recurrent Neural Networks. We test the reviewed models first on controlled synthetic tasks and then on different real datasets, covering important practical cases of study. We provide a general overview of the most important architectures and we define guidelines for configuring the recurrent networks to predict real-valued time series.},
  url           = {http://arxiv.org/abs/1705.04378},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/BianchiMKRJ17},
  file          = {:papers/An overview and comparative analysis of Recurrent Neural Networks for Short Term Load Forecasting.pdf:PDF},
  review        = {An overview of the use of Elman RNN, LSTM, GRU, NARX, and Echo state RNNs for load forecasting.
Found some reasonable variance between all architectures.
Does not consider S2S. 

Also gives a very nice overview of RNNs in general. },
  timestamp     = {Wed, 07 Jun 2017 14:43:10 +0200},
}

@Article{Flunkert2017,
  author      = {Valentin Flunkert and David Salinas and Jan Gasthaus},
  title       = {DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks},
  journal     = {CoRR},
  date        = {2017-04-13},
  eprint      = {1704.04110v2},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  abstract    = {A key enabler for optimizing business processes is accurately estimating the probability distribution of a time series future given its past. Such probabilistic forecasts are crucial for example for reducing excess inventory in supply chains. In this paper we propose DeepAR, a novel methodology for producing accurate probabilistic forecasts, based on training an auto-regressive recurrent network model on a large number of related time series. We show through extensive empirical evaluation on several real-world forecasting data sets that our methodology is more accurate than state-of-the-art models, while requiring minimal feature engineering.},
  file        = {:papers/DeepAR Forecasting Recurrent Networks.pdf:PDF},
  keywords    = {cs.AI, cs.LG, stat.ML},
}

@Article{Scott2014,
  author      = {Paul Scott and Sylvie Thi√©baux},
  title       = {Dynamic Optimal Power Flow in Microgrids using the Alternating Direction Method of Multipliers},
  journal     = {CoRR},
  year        = {2014},
  date        = {2014-10-29},
  eprint      = {1410.7868v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {Smart devices, storage and other distributed technologies have the potential to greatly improve the utilisation of network infrastructure and renewable generation. Decentralised control of these technologies overcomes many scalability and privacy concerns, but in general still requires the underlying problem to be convex in order to guarantee convergence to a global optimum. Considering that AC power flows are non-convex in nature, and the operation of household devices often requires discrete decisions, there has been uncertainty surrounding the use of distributed methods in a realistic setting. This paper extends prior work on the alternating direction method of multipliers (ADMM) for solving the dynamic optimal power flow (D-OPF) problem. We utilise more realistic line and load models, and introduce a two-stage approach to managing discrete decisions and uncertainty. Our experiments on a suburb-sized microgrid show that this approach provides near optimal results, in a time that is fast enough for receding horizon control. This work brings distributed control of smart-grid technologies closer to reality.},
  file        = {:papers/Dynamic Optimal Power Flow in Microgrids using the Alternating Direction Method of Multipliers.pdf:PDF},
  keywords    = {math.OC, cs.SY},
}

@Article{Evan2016,
  author  = {Evan Franklin},
  title   = {Agents of change Making batteries go the extra mile},
  journal = {ReNew, no. 136, pp. 56-58,},
  year    = {2016},
  file    = {:papers/agents-of-change-bruny.pdf:PDF},
}

@Report{Jacobs2017,
  author      = {Jacobs},
  title       = {Projections of uptake of small-scale systems (Australian Energy Market Operator)},
  type        = {resreport},
  institution = {AEMO},
  year        = {2017},
  date        = {2017-06-09},
  file        = {:papers/Projections of Uptake of Small-scale Systems.pdf:PDF},
}

@Report{AEMO2016,
  author      = {AEMO},
  title       = {2016 NATIONAL ELECTRICITY FORECASTING REPORT},
  type        = {resreport},
  institution = {AEMO},
  year        = {2016},
  file        = {:papers/2016 National Electricity Forecasting Report NEFR.pdf:PDF},
}

@Article{Kong2018,
  author    = {Weicong Kong and Zhao Yang Dong and David J. Hill and Fengji Luo and Yan Xu},
  title     = {Short-Term Residential Load Forecasting Based on Resident Behaviour Learning},
  journal   = {{IEEE} Transactions on Power Systems},
  year      = {2018},
  volume    = {33},
  number    = {1},
  month     = {jan},
  pages     = {1087--1088},
  doi       = {10.1109/tpwrs.2017.2688178},
  file      = {:papers/kong2017.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Thesis{Lier2015,
  author      = {Ciar√°n Lier},
  title       = {Applying Machine Learning Techniques to Short Term Load Forecasting},
  type        = {mathesis},
  institution = {University of Groningen},
  year        = {2015},
  file        = {:papers/Thesis_Ciaran_Lier.pdf:PDF},
}

@Article{Basil1975,
  author    = {Victor R. Basil and Albert J. Turner},
  title     = {Iterative enhancement: A practical technique for software development},
  journal   = {{IEEE} Transactions on Software Engineering},
  year      = {1975},
  volume    = {{SE}-1},
  number    = {4},
  month     = {dec},
  pages     = {390--396},
  doi       = {10.1109/tse.1975.6312870},
  file      = {:papers/Iterative enhancement A practical technique for software development.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Book{Box1970,
  author    = {George E. P. Box and Gwilym M. Jenkins and Gregory C. Reinsel},
  title     = {Time Series Analysis},
  year      = {1970},
  publisher = {John Wiley {\&} Sons, Inc.},
  file      = {:papers/Time Series Analysis_ Forecasting and Control-Wiley (2015).pdf:PDF},
  month     = {jun},
}

@Book{Weron2006,
  author    = {Rafa{\l} Weron},
  title     = {Modeling and Forecasting Electricity Loads and Prices},
  year      = {2006},
  publisher = {John Wiley {\&} Sons Ltd},
  doi       = {10.1002/9781118673362},
  file      = {:papers/Rafal Weron-Modeling and Forecasting Electricity Loads and Prices_ A Statistical Approach (The Wiley Finance Series) (2006).pdf:PDF},
  month     = {dec},
}

@Article{Ceperic2013,
  author    = {Ervin Ceperic and Vladimir Ceperic and Adrijan Baric},
  title     = {A Strategy for Short-Term Load Forecasting by Support Vector Regression Machines},
  journal   = {{IEEE} Transactions on Power Systems},
  year      = {2013},
  volume    = {28},
  number    = {4},
  month     = {nov},
  pages     = {4356--4364},
  doi       = {10.1109/tpwrs.2013.2269803},
  file      = {:papers/A Strategy for Short-Term Load Forecasting by Support Vector Regression Machines.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Drucker1996,
  author    = {Drucker, Harris and Burges, Chris J. C. and Kaufman, Linda and Smola, Alex and Vapnik, Vladimir},
  title     = {Support Vector Regression Machines},
  booktitle = {Proceedings of the 9th International Conference on Neural Information Processing Systems},
  year      = {1996},
  series    = {NIPS'96},
  publisher = {MIT Press},
  location  = {Denver, Colorado},
  pages     = {155--161},
  url       = {http://dl.acm.org/citation.cfm?id=2998981.2999003},
  acmid     = {2999003},
  address   = {Cambridge, MA, USA},
  file      = {:papers/Support Vector Regression Machines.pdf:PDF},
  numpages  = {7},
}

@Article{Chen2004,
  author    = {B.-J. Chen and M.-W. Chang and C.-J. Lin},
  title     = {Load Forecasting Using Support Vector Machines: A Study on {EUNITE} Competition 2001},
  journal   = {{IEEE} Transactions on Power Systems},
  year      = {2004},
  volume    = {19},
  number    = {4},
  month     = {nov},
  pages     = {1821--1830},
  doi       = {10.1109/tpwrs.2004.835679},
  file      = {:papers/Load Forecasting Using Support Vector Machines A Study on EUNITE Competition 2001.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Smola2004,
  author    = {Alex J. Smola and Bernhard Sch√∂lkopf},
  title     = {A tutorial on support vector regression},
  journal   = {Statistics and Computing},
  year      = {2004},
  volume    = {14},
  number    = {3},
  month     = {aug},
  pages     = {199--222},
  doi       = {10.1023/b:stco.0000035301.49549.88},
  file      = {:papers/A tutorial on support vector regression.pdf:PDF},
  publisher = {Springer Nature},
}

@Article{Desouky2000,
  author    = {A.A. El Desouky and M.M. El Kateb},
  title     = {Hybrid adaptive techniques for electric-load forecast using {ANN} and {ARIMA}},
  journal   = {{IEE} Proceedings - Generation, Transmission and Distribution},
  year      = {2000},
  volume    = {147},
  number    = {4},
  pages     = {213},
  doi       = {10.1049/ip-gtd:20000521},
  file      = {:papers/Hybrid adaptive techniques for electric-load forecast using ANN and ARIMA.pdf:PDF},
  publisher = {Institution of Engineering and Technology ({IET})},
}

@Article{Bennett2014,
  author    = {Christopher J. Bennett and Rodney A. Stewart and Jun Wei Lu},
  title     = {Forecasting low voltage distribution network demand profiles using a pattern recognition based expert system},
  journal   = {Energy},
  year      = {2014},
  volume    = {67},
  month     = {apr},
  pages     = {200--212},
  doi       = {10.1016/j.energy.2014.01.032},
  file      = {:papers/Bennett2014.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@InProceedings{Karthika2017,
  author    = {S Karthika and Vijaya Margaret and K. Balaraman},
  title     = {Hybrid short term load forecasting using {ARIMA}-{SVM}},
  booktitle = {2017 Innovations in Power and Advanced Computing Technologies (i-{PACT})},
  year      = {2017},
  publisher = {{IEEE}},
  month     = {apr},
  doi       = {10.1109/ipact.2017.8245060},
  file      = {:papers/Hybrid Short Term Load Forecasting using ARIMA-SVM.pdf:PDF},
}

@Article{RochaReis2005,
  author    = {A.J. RochaReis and A.P. AlvesdaSilva},
  title     = {Feature Extraction via Multiresolution Analysis for Short-Term Load Forecasting},
  journal   = {{IEEE} Transactions on Power Systems},
  year      = {2005},
  volume    = {20},
  number    = {1},
  month     = {feb},
  pages     = {189--198},
  doi       = {10.1109/tpwrs.2004.840380},
  file      = {:papers/Feature extraction via multiresolution analysis.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{AMJADY2009,
  author    = {N AMJADY and F KEYNIA},
  title     = {Short-term load forecasting of power systems by combination of wavelet transform and neuro-evolutionary algorithm},
  journal   = {Energy},
  year      = {2009},
  volume    = {34},
  number    = {1},
  month     = {jan},
  pages     = {46--57},
  doi       = {10.1016/j.energy.2008.09.020},
  file      = {:papers/Short-term load forecasting of power systems by combination of wavelet transform and neuro-evolutionary algorithm.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@Article{Deihimi2012,
  author    = {Ali Deihimi and Hemen Showkati},
  title     = {Application of echo state networks in short-term electric load forecasting},
  journal   = {Energy},
  year      = {2012},
  volume    = {39},
  number    = {1},
  month     = {mar},
  pages     = {327--340},
  doi       = {10.1016/j.energy.2012.01.007},
  file      = {:papers/Application of echo state networks in short-term electric load forecasting.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@Article{Elattar2010,
  author    = {Ehab E Elattar and John Goulermas and Q H Wu},
  title     = {Electric Load Forecasting Based on Locally Weighted Support Vector Regression},
  journal   = {{IEEE} Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  year      = {2010},
  volume    = {40},
  number    = {4},
  month     = {jul},
  pages     = {438--447},
  doi       = {10.1109/tsmcc.2010.2040176},
  file      = {:papers/Electric Load Forecasting Based on Locally Weighted Support Vector Regression.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Book{Goodfellow-et-al-2016,
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  title     = {Deep Learning},
  year      = {2016},
  note      = {\url{http://www.deeplearningbook.org}},
  publisher = {MIT Press},
  file      = {:papers/deeplearningbook.pdf:PDF},
}

@Book{negnevitsky2005artificial,
  author    = {Negnevitsky, Michael},
  title     = {Artificial intelligence: a guide to intelligent systems},
  year      = {2005},
  publisher = {Pearson Education},
  file      = {:papers/Artificial_Intelligence-A_Guide_to_Intelligent_Systems.pdf:PDF},
}

@InCollection{Bottou2011,
  author    = {L{\'{e}}on Bottou},
  title     = {Large-Scale Machine Learning with Stochastic Gradient Descent},
  booktitle = {Chapman {\&} Hall/{CRC} Computer Science {\&} Data Analysis},
  year      = {2011},
  publisher = {Chapman and Hall/{CRC}},
  pages     = {17--25},
  doi       = {10.1201/b11429-4},
  file      = {:papers/Large-Scale Machine Learning with Stochastic Gradient Descent.pdf:PDF},
  month     = {dec},
}

@Article{Zainal-Mokhtar2013,
  author    = {Khursiah Zainal-Mokhtar and Junita Mohamad-Saleh},
  title     = {An Oil Fraction Neural Sensor Developed Using Electrical Capacitance Tomography Sensor Data},
  journal   = {Sensors},
  year      = {2013},
  volume    = {13},
  number    = {9},
  month     = {aug},
  pages     = {11385--11406},
  doi       = {10.3390/s130911385},
  file      = {:papers/An Oil Fraction Neural Sensor Developed Using Electrical Capacitance Tomography Sensor Data.pdf:PDF;:images/A-schematic-diagram-of-a-Multi-Layer-Perceptron-MLP-neural-network.png:PNG image},
  publisher = {{MDPI} {AG}},
}

@WWW{Deshpande2017,
  author = {Mohit Deshpande},
  editor = {Perceptrons: The First Neural Networks},
  title  = {Perceptrons: The First Neural Networks},
  year   = {2017},
  date   = {2017-09-12},
  url    = {https://pythonmachinelearning.pro/perceptrons-the-first-neural-networks/},
  file   = {:images/Single-Perceptron-825x459.png:PNG image},
}

@Article{2017arXiv171009829S,
  author        = {{Sabour}, S. and {Frosst}, N. and {E Hinton}, G.},
  title         = {{Dynamic Routing Between Capsules}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = oct,
  eprint        = {1710.09829},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171009829S},
  archiveprefix = {arXiv},
  file          = {:papers/Dynamic Routing Between Capsules.pdf:PDF},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass  = {cs.CV},
}

@Article{Vaswani2017,
  author      = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title       = {Attention Is All You Need},
  year        = {2017},
  date        = {2017-06-12},
  eprint      = {1706.03762v5},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  abstract    = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  file        = {:papers/Attention Is All You Need.pdf:PDF},
  keywords    = {cs.CL, cs.LG},
}

@Article{Chiu2017,
  author      = {Chung-Cheng Chiu and Tara N. Sainath and Yonghui Wu and Rohit Prabhavalkar and Patrick Nguyen and Zhifeng Chen and Anjuli Kannan and Ron J. Weiss and Kanishka Rao and Ekaterina Gonina and Navdeep Jaitly and Bo Li and Jan Chorowski and Michiel Bacchiani},
  title       = {State-of-the-art Speech Recognition With Sequence-to-Sequence Models},
  year        = {2017},
  date        = {2017-12-05},
  eprint      = {1712.01769v6},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  abstract    = {Attention-based encoder-decoder architectures such as Listen, Attend, and Spell (LAS), subsume the acoustic, pronunciation and language model components of a traditional automatic speech recognition (ASR) system into a single neural network. In previous work, we have shown that such architectures are comparable to state-of-theart ASR systems on dictation tasks, but it was not clear if such architectures would be practical for more challenging tasks such as voice search. In this work, we explore a variety of structural and optimization improvements to our LAS model which significantly improve performance. On the structural side, we show that word piece models can be used instead of graphemes. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy. We present results with a unidirectional LSTM encoder for streaming recognition. On a 12, 500 hour voice search task, we find that the proposed changes improve the WER from 9.2% to 5.6%, while the best conventional system achieves 6.7%; on a dictation task our model achieves a WER of 4.1% compared to 5% for the conventional system.},
  file        = {:papers/STATE-OF-THE-ART SPEECH RECOGNITION WITH SEQUENCE-TO-SEQUENCE MODELS.pdf:PDF},
  keywords    = {cs.CL, cs.SD, eess.AS, stat.ML},
}

@Article{DBLP:journals/corr/OordDZSVGKSK16,
  author        = {A{\"{a}}ron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alex Graves and Nal Kalchbrenner and Andrew W. Senior and Koray Kavukcuoglu},
  title         = {WaveNet: {A} Generative Model for Raw Audio},
  journal       = {CoRR},
  year          = {2016},
  volume        = {abs/1609.03499},
  eprint        = {1609.03499},
  url           = {http://arxiv.org/abs/1609.03499},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/OordDZSVGKSK16},
  file          = {:papers/WAVENET A GENERATIVE MODEL FOR RAW AUDIO.pdf:PDF},
  timestamp     = {Wed, 07 Jun 2017 14:42:54 +0200},
}

@Article{Mandal2010,
  author    = {Paras Mandal and Anurag K. Srivastava and Tomonobu Senjyu and Michael Negnevitsky},
  title     = {A new recursive neural network algorithm to forecast electricity price for {PJM} day-ahead market},
  journal   = {International Journal of Energy Research},
  year      = {2010},
  volume    = {34},
  number    = {6},
  month     = {may},
  pages     = {507--522},
  doi       = {10.1002/er.1569},
  file      = {:papers/A new recursive neural network algorithm to forecast electricity price for PJM day-ahead market.pdf:PDF},
  publisher = {Wiley},
}

@Article{Zhao2017,
  author    = {Zheng Zhao and Weihai Chen and Xingming Wu and Peter C. Y. Chen and Jingmeng Liu},
  title     = {{LSTM} network: a deep learning approach for short-term traffic forecast},
  journal   = {{IET} Intelligent Transport Systems},
  year      = {2017},
  volume    = {11},
  number    = {2},
  month     = {mar},
  pages     = {68--75},
  doi       = {10.1049/iet-its.2016.0208},
  file      = {:papers/LSTM network a deep learning approach for short-term traffic forecast.pdf:PDF},
  publisher = {Institution of Engineering and Technology ({IET})},
}

@Article{Chen2010,
  author    = {Ying Chen and P.B. Luh and Che Guan and Yige Zhao and L.D. Michel and M.A. Coolbeth and P.B. Friedland and S.J. Rourke},
  title     = {Short-Term Load Forecasting: Similar Day-Based Wavelet Neural Networks},
  journal   = {{IEEE} Transactions on Power Systems},
  year      = {2010},
  volume    = {25},
  number    = {1},
  month     = {feb},
  pages     = {322--330},
  doi       = {10.1109/tpwrs.2009.2030426},
  file      = {:papers/Short-Term Load Forecasting Similar Day-Based Wavelet Neural Networks.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Kong2017,
  author    = {Weicong Kong and Zhao Yang Dong and Youwei Jia and David J. Hill and Yan Xu and Yuan Zhang},
  title     = {Short-Term Residential Load Forecasting based on {LSTM} Recurrent Neural Network},
  journal   = {{IEEE} Transactions on Smart Grid},
  year      = {2017},
  pages     = {1--1},
  doi       = {10.1109/tsg.2017.2753802},
  file      = {:papers/Short-Term Residential Load Forecasting based on {LSTM} Recurrent Neural Network.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Comment{jabref-meta: databaseType:biblatex;}
